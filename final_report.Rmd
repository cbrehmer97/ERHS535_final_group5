---
title: "Final Report"
author: "Nikki Johnson, Molly Hischke, Collin Brehmer, and Kyle Hancock"
date: "12/9/2019"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = FALSE, message = FALSE)
```

# Introduction: Research question, and Why did you decide to ask this question?

The Daily Double is an important part of the game of Jeopardy. If a player can effectively predict where the Daily Doubles will be on the board they have a clear advantage to win the game. Recent Jeopardy contestant, James Holzhauer, became known for his strategy of hopping around the board to find the Daily Doubles and then betting larger than normal quantities. In just 32 games, his total winnings reached almost $2.5 million which is an average of over $75,000 per game.

What positions on the Jeopardy board are most likely to be a Daily Double? Several people have attempted to answer this question recently and their results can be found in a variety of different sources. Finding the answer to this question is not as simple as it seems at first. The common datasets for Jeopardy questions do not contain the questions that were not asked. This can make finding the exact x and y positions difficult. Multiple approaches can be taken to deal with this issue. The approach taken to develop the figures in this report are described in the methods section. The answer to this question can be effectively presented visually through a heatmap that shows each position on the Jeopardy board. This important question presents a challenge in data cleaning, but when done correctly can result in interesting figures that has drawn the attention of many Jeopardy fans and data enthusiasts.

# Methods: 

How did you investigate the data to try to answer your question? This should not include R code (save that for the tutorial part), but rather should use language like “To determine if … was associated with …, we measured the correlation …”. It’s fine for this project if the Methods are fairly simple (“We investigated the distribution of … using boxplots …”, “We took the mean and interquartile range of …”, “We mapped state-level averages of …”, etc.). Why do you choose to use the Methods you used? Why do you think they’re appropriate and useful for your project?
```{r, echo = FALSE}
library(tidyverse)
library(ggplot2)
jeopardy_board <- tibble(
  x_pos = c("Category 1", "Category 1", "Category 1", 
            "Category 1", "Category 1", 
            "Category 2", "Category 2", "Category 2", 
            "Category 2", "Category 2",
            "Category 3", "Category 3", "Category 3", 
            "Category 3", "Category 3", 
            "Category 4", "Category 4", "Category 4", 
            "Category 4", "Category 4", 
            "Category 5", "Category 5", "Category 5", 
            "Category 5", "Category 5",
            "Category 6", "Category 6", "Category 6", 
            "Category 6", "Category 6"),
  y_pos = c(200, 400, 600, 800, 1000, 
            200, 400, 600, 800, 1000,
            200, 400, 600, 800, 1000,
            200, 400, 600, 800, 1000,
            200, 400, 600, 800, 1000, 
            200, 400, 600, 800, 1000))

jeopardy_board <- jeopardy_board %>% 
  mutate(number_of_dd = c(1, 0, 0, 2, 1,
                          1, 0, 1, 1, 1,
                          1, 1, 1, 1, 1,
                          1, 1, 1, 0, 1,
                          1, 0, 0, 1, 1,
                          1, 1, 1, 0, 1),
          text = c(NA, "MISSING", "MISSING", "Daily Double", NA,
                          NA, "MISSING", NA, NA, NA,
                          NA, NA, NA, NA, NA,
                          NA, NA, NA, "MISSING", NA,
                          NA, "MISSING", "MISSING", NA, NA,
                          NA, NA, NA, "MISSING", NA)) 
weighted_jeopardy_board <- jeopardy_board %>% 
  mutate(number_of_dd = c(NA, 0.33, 0.33, 0.33, NA,
                          NA, NA, NA, NA, NA,
                          NA, NA, NA, NA, NA,
                          NA, NA, NA, NA, NA,
                          NA, NA, NA, NA, NA,
                          NA, NA, NA, NA, NA)) 

```

There were two main challenges in finding the positions of the daily double questions:

1. Finding the x position (the location of the categoires).
2. Finding the y position (the location of the values). 

Not all categories per episode (n = 12) were available in the dataset. To solve this issue, only episodes where all 12 categories were played were included. The assumption was that the dataset listed the categories in order by location per episode (i.e. the first category listed in the data set would be the leftmost category on the game board for each episode). 

In addition, not every question (n = 5) within a category was availabe in the dataset. This was espeically challenging when a category contained a daily double question. Since players select their own bid for daily doubles, the value within the dataset gave little information about the location on the y axis. If there were 2 missing questions and 1 daily double within a category, the daily double could be at any one of those locations (Figure 1). To rectify this issue, the chance that the daily double appeared at a given y position was weighted equally across all missing questions in the category (Figure 2). 
      
```{r, echo = FALSE, fig.height= 4, fig.width = 7}
jeopardy_board %>% 
   mutate(y_pos = factor(y_pos), 
                        y_pos = factor(y_pos, levels = rev(levels(y_pos)))) %>% 
  ggplot(aes(x = x_pos, y = y_pos, fill = number_of_dd)) +
  geom_tile(color = "White", size = 0.1) +
  scale_fill_continuous(high = "#132B43", low = "white") + 
  labs(x = NULL, 
       y = NULL, 
       fill = NULL) +
  theme(legend.position = "none") + 
  geom_text(aes(label = text), color = "gray") +
  scale_x_discrete(position = "top") +
  labs(caption = paste0("Figure 1. In Category 1, the 400, 600 and 800 values",
  " were missing, and \na daily double question was asked. But it's unknown ",
  "if the daily double was \nat the 400, 600 or 800 value position.")) +
  theme(plot.caption = element_text(hjust = 0, size = rel(1.2)))
```

```{r, echo = FALSE, fig.height= 4, fig.width = 7}
weighted_jeopardy_board %>% 
   mutate(y_pos = factor(y_pos), 
                        y_pos = factor(y_pos, levels = rev(levels(y_pos)))) %>% 
  ggplot(aes(x = x_pos, y = y_pos, fill = number_of_dd)) +
  geom_tile(color = "White", size = 0.1) +
  scale_fill_continuous(high = "#132B43", low = "white") + 
  labs(x = NULL, 
       y = NULL, 
       fill = NULL) +
  theme(legend.position = "none") + 
  geom_text(aes(label = number_of_dd)) +
  scale_x_discrete(position = "top") +
  labs(caption = paste0("Figure 2. The daily double within a category was ",
                        "weighted acrossed all \nmissing values.")) +
  theme(plot.caption = element_text(hjust = 0, size = rel(1.2)))
```

# Results: 
What did you find out? Most of these slides should be figures or tables. Discuss your interpretation of your results as you present them. Ideally, you should be able to show your main results in about 3 slides, with one figure or table per slide.

<<<<<<< HEAD
The heatmaps for all years, separated by rounds, displays interesting results. In round 1, the daily double most often occurs in category 1, point value 800 (7.33%). Other “hot” points are category 4, value 800 (6.81%) and category 1, value 1000 (6.75%). The least likely placements are across all categories in value 100 (0.35% or less) and in categories 2 (4.63% or less) and 6 (4.27% or less). Value 800 is favored across all categories (4.27% – 7.33%). In round 2, the daily double is most often located in category 1, value 1600 (7.06%). The other common positions are category 5, value 1600 (6.59%) and category 3, value 1600 (6.41%). The least likely occurrences are across value 100 (0.51% or less) and down category 2 (4.78% or less) and category 6 (4.75% or less). Value 1600 is elected for the position most often across categories (4.75% – 7.06%). Overall, the occurrence of the daily double remains extremely similar across round 1 and round 2 for the years analyzed. These results are visually presented in Figure 3 (round 1) and Figure 4 (round 2).

As a secondary question, the heatmaps for all data (rounds 1 and 2) were faceted by year to look at changes over time. Since the rounds were similar in the initail analysis, the data was not seperated by round. The results for years 2001 - 2015 look similar to the results by round. The "hot" points are similar as well as the overall pattern. In 2016, there seems to be a shift away from the traditional spots. Previously, catergory 2 was one of the least likely columns but after 2016 the percentage surpassed the percentages in catergory 1 (previosuly, the most likely column). There also seems to be a shift from row 4 to row 3. Before 2016, rows 4 and 5 were the most likely to contain a Daily Double. After 2016, row 3 a higher percentage of Daily Doubles. The pattern after 2016 also seems to have more randomness than prior to 2016. 


```{r, warning = FALSE, error = FALSE, message = FALSE, echo= FALSE}
library(readr)
library(plotly)
library(forcats)
library(lubridate)
library(tidyr)
library(purrr)


dd <- read_csv("data/data_final.csv") 
dd <- dd %>% 
  rename("dd_weighting" = "weight") %>% 
  mutate(y_pos = factor(y_pos, levels = c(1, 2, 3, 4, 5)),
         y_pos = factor(y_pos, levels = rev(levels(y_pos))),
         x_pos = factor(x_pos, levels = c(1, 2, 3, 4, 5, 6)),
         x_pos = fct_recode(x_pos, 
                            "Category 1" = "1",
                            "Category 2" = "2",
                            "Category 3" = "3",
                            "Category 4" = "4",
                            "Category 5" = "5",
                            "Category 6" = "6")) 
```

```{r Daily Doubles by Round, echo= FALSE}
dd_1 <- dd %>% 
  filter(round == "1") %>%
  mutate(y_pos = fct_recode(y_pos,
                            "200" = "1",
                            "400" = "2",
                            "600" = "3",
                            "800" = "4",
                            "1000" = "5")) %>% 
  group_by(x_pos, y_pos) %>% 
  summarise(number_of_doubles = sum(dd_weighting)) %>% 
  mutate(Percent = round(number_of_doubles/2476*100, digits = 2)) %>%
  ggplot(aes(x = x_pos, y = y_pos, fill = Percent)) +
  geom_tile(color = "white") +
   labs(x = NULL, 
        y = NULL, 
        fill = "Percent of Daily Doubles") +
  scale_fill_continuous(high = "dodgerblue4", 
                        low = "azure") +
  theme(legend.position = "bottom", 
        legend.box = "horizontal") +
  geom_text(aes(label = paste0(Percent, "%")), color = "black") +
  scale_x_discrete(position = "top")
dd_1
```
\
Figure 3. Percent of Daily Doubles by Position in Round 1
\
\
\
\
```{r echo= FALSE}
dd_2 <- dd %>%
  filter(round == "2") %>%
  mutate(y_pos = fct_recode(y_pos,
                            "400" = "1",
                            "800" = "2",
                            "1200" = "3",
                            "1600" = "4",
                            "2000" = "5")) %>% 
  group_by(x_pos, y_pos) %>% 
  summarise(number_of_doubles = sum(dd_weighting)) %>% 
  mutate(Percent = round(number_of_doubles/4887*100, digits = 2)) %>%
    ggplot(aes(x = x_pos, y = y_pos, fill = Percent)) +
  geom_tile(color = "white") +
   labs(x = NULL, 
        y = NULL, 
        fill = "Percent of Daily Doubles") +
  scale_fill_continuous(high = "dodgerblue4", 
                        low = "azure") +
  theme(legend.position = "bottom", 
        legend.box = "horizontal") +
  geom_text(aes(label = paste0(Percent, "%")), color = "black") +
  scale_x_discrete(position = "top")
dd_2

```
\
Figure 4. Percent of Daily Doubles by Position in Round 2
\
\
\
\
```{r Daily Double Heatmap by year, fig.height = 7, echo= FALSE}
dd %>% 
  mutate(year = year(air_date),
         y_pos = fct_recode(y_pos,
                            "200/400" = "1",
                            "400/800" = "2",
                            "600/1200" = "3",
                            "800/1600" = "4",
                            "1000/2000" = "5")) %>% 
  group_by(x_pos, y_pos, year) %>%
  summarise(number_of_doubles = sum(dd_weighting)) %>% 
  ungroup() %>% 
  group_by(year) %>% 
  mutate(Percent = round(number_of_doubles/sum(number_of_doubles) * 100), 
         digits = 2)  %>% 
  ggplot(aes(x = x_pos, y = y_pos, fill = Percent)) +
  geom_tile(color = "white") +
   labs(x = NULL, 
        y = NULL, 
        fill = "Percent of Daily Doubles") +
  scale_fill_continuous(high = "dodgerblue4", 
                        low = "azure") +
  theme(legend.position = "bottom", 
        legend.box = "horizontal",
        axis.text.x = element_text(angle = 90)) +
  scale_x_discrete(position = "top") +
  facet_wrap(~year, ncol = 4)

```
\
Figure 5. Percent of Daily Doubles by Position Faceted by Year
\
\
\
\

## Conclusions
The results of this analysis are very similar to previous analyses on Daily Double location by round. One of the original analyses of Daily Double location was conducted by Nathan Yau (https://flowingdata.com/2015/03/03/where-to-find-jeopardy-daily-doubles/) in 2015. The results of this study look similar to his heatmap that shows the location of all Daily Doubles for 31 seasons. Yau's analysis was done because of a Jeopardy contestant, Arthur Chu, that appeared on the show in 2014. Arthur Chu found success on the show by jumping around the board hunting for the Daily Doubles. He was not the first to adopt this strategy, but was one of the most controversial players because of his approach. Since 2015, there have been other analyses that looked at the same question and found similar results by round. To our knowledge, no other analyses have look at the data by year. This analysis included data up to 2019 and showed a noticable shift in location around 2015/2016. This shift likely occured because of Arthur Chu and the subsequent analyses of Daily Double locations. This change should be analyzed over the next few years to see if another pattern is established or if Jeopardy adopts a more random Daily Double placement.


=======
# Conclusions: 
So what? How do your results compare with what other people have found out about your research question? Based on what you found, are there now other things you want to check out?
>>>>>>> ec7b46fb65b8fbbebade3dd262e1174866b48e4a
